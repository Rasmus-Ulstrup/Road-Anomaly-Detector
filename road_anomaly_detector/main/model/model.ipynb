{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crackscope/miniconda3/envs/road_anomaly_detector/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/crackscope/miniconda3/envs/road_anomaly_detector/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data directories\n",
    "\n",
    "# #Cracktree200\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/cracktree200/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/cracktree200/Masks\")\n",
    "\n",
    "#Forest\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/forest/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/forest/Masks\")\n",
    "\n",
    "# #Gaps384\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/GAPS384/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/GAPS384/Masks\")\n",
    "\n",
    "# #CFD\n",
    "image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/CrackForestDataset_(CFD)/Images\")\n",
    "mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/CrackForestDataset_(CFD)/Masks\")\n",
    "\n",
    "# #Mixed\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/Mixed/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/Mixed/Masks\")\n",
    "\n",
    "# Get list of image and mask file paths\n",
    "image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
    "mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
    "\n",
    "# Split data\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((448, 448)),\n",
    "    transforms.Resize((448, 320)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Set batch size for flexibility\n",
    "batch_size = 8  # Change this value as needed\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define UNet Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.Conv2d(256, 256, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Sequential(nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(), nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Output\n",
    "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e1p = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.encoder2(e1p)\n",
    "        e2p = self.pool2(e2)\n",
    "        \n",
    "        b = self.bottleneck(e2p)\n",
    "        \n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "        \n",
    "        return torch.sigmoid(self.conv_last(d1))\n",
    "\n",
    "model = UNet().cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training simple model\n",
    "# Dice Loss function\n",
    "def dice_loss(pred, target, smooth=1e-5):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    dice = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop with model saving\n",
    "num_epochs = 500\n",
    "model_save_path = \"advanced_unet_segmentation_model.pth\"  # Path to save the trained model\n",
    "#model_save_path = \"simple_unet_segmentation_model_cracktree200.pth\"  # Path to save the trained model\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initial best validation loss\n",
    "epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.cuda(), masks.cuda()\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = dice_loss(outputs, masks)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_images, val_masks in val_loader:\n",
    "            val_images, val_masks = val_images.cuda(), val_masks.cuda()\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += dice_loss(val_outputs, val_masks)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        print(\"Model improved\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_without_improvement} epochs.\")\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"No improvement in validation loss for {epochs_without_improvement} epochs.\")\n",
    "        break\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U-net structure 56 Layers 30 Encoder + 24 decoder + 2 output\n",
    "# Paths to the data directories\n",
    "\n",
    "# #Cracktree200\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/cracktree200/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/cracktree200/Masks\")\n",
    "\n",
    "#Forest\n",
    "image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/forest/Images\")\n",
    "mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/forest/Masks\")\n",
    "\n",
    "# #Gaps384\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/GAPS384/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/GAPS384/Masks\")\n",
    "\n",
    "# #CFD\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/CrackForestDataset_(CFD)/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/CrackForestDataset_(CFD)/Masks\")\n",
    "\n",
    "# #Mixed\n",
    "# image_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/Mixed/Images\")\n",
    "# mask_dir = os.path.expanduser(r\"~/Documents/datasetz/datasets/Mixed/Masks\")\n",
    "\n",
    "# Get list of image and mask file paths\n",
    "image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
    "mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
    "\n",
    "# Split data\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Set batch size for flexibility\n",
    "batch_size = 8  # Change this value as needed\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self._conv_block(1, 64)\n",
    "        self.encoder2 = self._conv_block(64, 128)\n",
    "        self.encoder3 = self._conv_block(128, 256)\n",
    "        self.encoder4 = self._conv_block(256, 512)\n",
    "        self.encoder5 = self._conv_block(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv5 = self._upconv_block(1024, 512)\n",
    "        self.decoder5 = self._conv_block(1024, 512)\n",
    "        self.upconv4 = self._upconv_block(512, 256)\n",
    "        self.decoder4 = self._conv_block(512, 256)\n",
    "        self.upconv3 = self._upconv_block(256, 128)\n",
    "        self.decoder3 = self._conv_block(256, 128)\n",
    "        self.upconv2 = self._upconv_block(128, 64)\n",
    "        self.decoder2 = self._conv_block(128, 64)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        \n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "    \n",
    "    def _upconv_block(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        e5 = self.encoder5(e4)\n",
    "        \n",
    "        # Decoder path\n",
    "        d5 = self.upconv5(e5)\n",
    "        d5 = torch.cat((d5, e4), dim=1)\n",
    "        d5 = self.decoder5(d5)\n",
    "        \n",
    "        d4 = self.upconv4(d5)\n",
    "        d4 = torch.cat((d4, e3), dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "        \n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e2), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "        \n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e1), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output_conv(d2)\n",
    "        out = self.output_activation(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = UNet().cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training advanced model\n",
    "def dice_loss(pred, target, smooth=1e-5):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    dice = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop with model saving\n",
    "num_epochs = 500\n",
    "model_save_path = \"advanced_unet_segmentation_model.pth\"  # Path to save the trained model\n",
    "#model_save_path = \"simple_unet_segmentation_model_cracktree200.pth\"  # Path to save the trained model\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initial best validation loss\n",
    "epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.cuda(), masks.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = dice_loss(outputs, masks)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_images, val_masks in val_loader:\n",
    "            val_images, val_masks = val_images.cuda(), val_masks.cuda()\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += dice_loss(val_outputs, val_masks)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        print(\"Model improved\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_without_improvement} epochs.\")\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"No improvement in validation loss for {epochs_without_improvement} epochs.\")\n",
    "        break\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference function\n",
    "def run_inference(model, image_path, transform=transform):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        input_tensor = transform(image).unsqueeze(0).cuda()  # Add batch dimension and move to GPU\n",
    "        output = model(input_tensor)\n",
    "        output = output.squeeze().cpu().numpy()  # Remove batch dimension and move to CPU\n",
    "        return output  # Returns the output mask as a NumPy array\n",
    "\n",
    "# Example usage for inference\n",
    "# Replace 'sample_image_path' with an actual path to an image you want to test\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/exposure_25_outdoor_pavement_cropped.png'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/forest_003.jpg'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/forest_041.jpg'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/GAPS384_train_0451_541_641.jpg'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/cracktree200_6224.jpg'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/cracktree200_6240.jpg'\n",
    "#sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/cracktree200_6620.jpg'\n",
    "sample_image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/cracktree200_6726.jpg'\n",
    "output_mask = run_inference(model, sample_image_path)\n",
    "\n",
    "# Displaying the result using matplotlib (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(Image.open(sample_image_path).convert(\"L\"), cmap='gray')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(output_mask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_large_image(image_path, width=2048):\n",
    "    \"\"\"\n",
    "    Load a large image with a fixed width and variable height.\n",
    "    Returns the image in grayscale mode as a NumPy array.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    image_np = np.array(image)\n",
    "    return image_np\n",
    "\n",
    "def sliding_window_inference(model, image_np, patch_height=448, overlap=0):\n",
    "    \"\"\"\n",
    "    Perform inference on large images using a sliding window approach.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model\n",
    "    - image_np: NumPy array of the image (grayscale)\n",
    "    - patch_height: Height of each patch to process\n",
    "    - overlap: Number of pixels to overlap between patches\n",
    "\n",
    "    Returns:\n",
    "    - Full segmentation mask for the original image\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the original image height\n",
    "    height, width = image_np.shape\n",
    "    full_mask = np.zeros_like(image_np, dtype=np.float32)\n",
    "    \n",
    "    # Sliding window loop\n",
    "    step = patch_height - overlap\n",
    "    for start_y in range(0, height, step):\n",
    "        end_y = min(start_y + patch_height, height)  # Ensure we don't exceed the image height\n",
    "        patch = image_np[start_y:end_y, :]\n",
    "\n",
    "        # Resize patch if it's smaller than the patch_height due to reaching the bottom of the image\n",
    "        if patch.shape[0] < patch_height:\n",
    "            padded_patch = np.zeros((patch_height, width), dtype=np.float32)\n",
    "            padded_patch[:patch.shape[0], :] = patch\n",
    "            patch_tensor = torch.tensor(padded_patch, dtype=torch.float32).unsqueeze(0).unsqueeze(0).cuda()\n",
    "        else:\n",
    "            patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).unsqueeze(0).cuda()\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(patch_tensor)\n",
    "        \n",
    "        # Remove batch and channel dimensions, and move to CPU\n",
    "        output_np = output.squeeze().cpu().numpy()\n",
    "\n",
    "        # If patch was padded, remove the padding from the output\n",
    "        if patch.shape[0] < patch_height:\n",
    "            output_np = output_np[:patch.shape[0], :]\n",
    "\n",
    "        # Store the output in the full mask\n",
    "        full_mask[start_y:end_y, :] = output_np[:end_y - start_y, :]\n",
    "        \n",
    "        # Break if we've reached the bottom\n",
    "        if end_y == height:\n",
    "            break\n",
    "    \n",
    "    return full_mask\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/crackscope/Road-Anomaly-Detector/road_anomaly_detector/main/model/test_images/exposure_25_outdoor_pavement_cropped.png'\n",
    "image_np = load_large_image(image_path)\n",
    "full_mask = sliding_window_inference(model, image_np)\n",
    "\n",
    "# Visualize result (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Input Image Segment\")\n",
    "plt.imshow(image_np, cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Predicted Mask Segment\")\n",
    "plt.imshow(full_mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_anomaly_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
